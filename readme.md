Transformers:

+ [Thinking Like Transformers](https://srush.github.io/raspy/)
+ [An Intuitive Introduction to Transformers](https://blog.paperspace.com/attention-is-all-you-need-the-components-of-the-transformer/)

Self-Attention:

+ [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)
+ [Understanding Attention Mechanism in Transformer Neural Networks](https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/)

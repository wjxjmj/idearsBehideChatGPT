# Overview

+ [What Is ChatGPT Doing and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)

# Transformers:

+ [Thinking Like Transformers](https://srush.github.io/raspy/)
+ [Transformers from Scratch](https://e2eml.school/transformers.html)
+ [An Intuitive Introduction to Transformers](https://blog.paperspace.com/attention-is-all-you-need-the-components-of-the-transformer/)

# Self-Attention:
+ [Twitter to: what the fuck is "attention," exactly](https://twitter.com/docmilanfar/status/1625334889678798848?s=20&t=Pel5zHcFYt2k1XASODMBZQ) 
+ [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)
+ [Understanding Attention Mechanism in Transformer Neural Networks](https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/)
+ [[draft] Note 10: Self-Attention & Transformers](http://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)
